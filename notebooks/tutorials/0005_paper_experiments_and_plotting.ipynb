{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For licensing see accompanying LICENSE file.\n",
    "# Copyright (C) 2025 Apple Inc. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper experiments re-production instructions\n",
    "\n",
    "In this notebook we produce the plots for the paper based on experimental runs\n",
    "in the `project-agent-evaluator` repository.\n",
    "\n",
    "The general workflow is as follows:\n",
    "\n",
    "1. Run a set of experiments on a dataset (each configuration corresponds to a different agent(s))\n",
    "2. Generate synthetic agents combining all baselines and the agent runs.\n",
    "4. Create plots, combining all of these runs.\n",
    "\n",
    "Note: we do not necessarily recommend running the experiments directly out of this notebooks. Instead it may be more useful to copy the relevant commands into a shell script, e.g.`experiments.sh`, and then running the script in the background, e.g `nohup bash experiments.sh &`. The purpose of this notebook is to provide all relevant commands in a single place. Good luck with the experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import re\n",
    "from typing import List, Literal\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up main path to results\n",
    "\n",
    "DIR = pathlib.Path(\"./paper_results/\")\n",
    "# assert not DIR.exists(), f\"Results directory already exist, this will overwrite them! ({DIR.absolute()})\"\n",
    "# os.makedirs(DIR.absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Main experiments on specific domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(s):\n",
    "    # Insert underscores before any uppercase letter that is followed by a lowercase letter\n",
    "    s = re.sub(r'(?<!^)(?=[A-Z])', '_', s)\n",
    "    # Convert the entire string to lowercase\n",
    "    return s.lower()\n",
    "\n",
    "def print_ageval_cmd(exp_config_path: str, experiment_name: str, *, output_file: str | None = None, data_path: str | None = None, data_name: str | None = None, n_seeds: int | None = None, prompt_type: Literal[\"arena_hard\"] | None = None, other_args: List[str] | None = None) -> str:\n",
    "    exp_file_name = to_snake_case(os.path.basename(exp_config_path))\n",
    "    experiment_name = to_snake_case(experiment_name)\n",
    "    hydra_dir = os.path.join(DIR.absolute(), experiment_name, output_file or exp_file_name)\n",
    "    cmd = f\"ageval -m -cd {exp_config_path} hydra.sweep.dir={hydra_dir}\"\n",
    "    if data_path:\n",
    "        assert data_name is None\n",
    "        cmd += f\" data_name=null data_path=\\\"{data_path}\\\"\"\n",
    "    elif data_name:\n",
    "        assert data_path is None\n",
    "        cmd += f\" data_path=null data_name=\\\"{data_name}\\\"\"\n",
    "    if n_seeds is not None:\n",
    "        dummy_value = \",\".join([str(i) for i in range(n_seeds)])\n",
    "        cmd += f\" dummy=\\\"{dummy_value}\\\"\"\n",
    "    if prompt_type is not None:\n",
    "        cmd += f\" evaluator_kwargs.prompt_type=\\\"{prompt_type}\\\"\"\n",
    "    if other_args:\n",
    "        processed_other_args = []\n",
    "        for other_arg in other_args:\n",
    "            processed_other_arg = other_arg.replace('\"', '\\\\\"')\n",
    "            equal_split = processed_other_arg.split(\"=\")\n",
    "            remaining_equals = \"=\".join(equal_split[1:])\n",
    "            processed_other_arg = f'{equal_split[0]}=\"{remaining_equals}\"'\n",
    "            processed_other_args.append(processed_other_arg)\n",
    "        cmd += \" \" + \" \".join(processed_other_args)\n",
    "    \n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: LongFact\n",
    "exp_name = \"LongFact\"\n",
    "long_fact = partial(print_ageval_cmd, experiment_name=exp_name)\n",
    "# gpt4o baselines\n",
    "long_fact(\"exp/configs/2024_08_22_paper/001_longfact/001_gpt4o_baselines\")\n",
    "# alpacaeval baseline\n",
    "long_fact(\"exp/configs/2024_08_22_paper/001_longfact/002_ae_baseline\")\n",
    "# gpt35 baseline and agent\n",
    "long_fact(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_baseline\")\n",
    "long_fact(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_agent\")\n",
    "# gpt4o agent\n",
    "long_fact(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\")\n",
    "# gemini baseline (currently very rate limited, need other key to run these experiments)\n",
    "# long_fact(\"exp/configs/2024_08_22_paper/001_longfact/003_gemini_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: GSM8k maths\n",
    "exp_name=\"gsm8k_cot_hard\"\n",
    "data_path=\"./data/external/gsm8k/gsm8k_cot_hard.csv\"\n",
    "gsm8k_math = partial(print_ageval_cmd, experiment_name=exp_name, data_path=data_path)\n",
    "# gpt4o baselines\n",
    "gsm8k_math(\"exp/configs/2024_08_22_paper/001_longfact/001_gpt4o_baselines\")\n",
    "# alpacaeval baseline\n",
    "gsm8k_math(\"exp/configs/2024_08_22_paper/001_longfact/002_ae_baseline\")\n",
    "# gpt35 baseline and agent\n",
    "gsm8k_math(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_baseline\")\n",
    "gsm8k_math(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_agent\")\n",
    "# gpt4o agent\n",
    "gsm8k_math(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\")\n",
    "# gemini baseline (currently very rate limited, need other key to run these experiments)\n",
    "# gsm8k_math(\"exp/configs/2024_08_22_paper/001_longfact/003_gemini_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: APPS\n",
    "exp_name=\"apps_competitive_gpt4\"\n",
    "data_path=\"data/external/code_apps/competitive_gpt4.csv\"\n",
    "apps = partial(print_ageval_cmd, experiment_name=exp_name, data_path=data_path)\n",
    "# gpt4o baselines\n",
    "apps(\"exp/configs/2024_08_22_paper/001_longfact/001_gpt4o_baselines\")\n",
    "# alpacaeval baseline\n",
    "apps(\"exp/configs/2024_08_22_paper/001_longfact/002_ae_baseline\")\n",
    "# gpt35 baseline and agent\n",
    "apps(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_baseline\")\n",
    "apps(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_agent\")\n",
    "# gpt4o agent\n",
    "apps(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\")\n",
    "# gemini baseline (currently very rate limited, need other key to run these experiments)\n",
    "# apps(\"exp/configs/2024_08_22_paper/001_longfact/003_gemini_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Experiments on general domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Rewardbench non-code, non-maths\n",
    "# gpt4o baselines\n",
    "data_name=\"alpacaeval-easy, alpacaeval-hard, alpacaeval-length, donotanswer, llmbar-adver-GPTInst, llmbar-adver-GPTOut, llmbar-adver-manual, llmbar-adver-neighbor, llmbar-natural, mt-bench-easy, mt-bench-hard, mt-bench-med, refusals-dangerous, refusals-offensive, xstest-should-refuse, xstest-should-respond\"\n",
    "exp_name=\"rewardbench_general\"\n",
    "n_seeds=3  # this is a large amount of data, less seeds necessary to obtain low variance estimates\n",
    "rewardbench_general = partial(print_ageval_cmd, experiment_name=exp_name, data_name=data_name, n_seeds=n_seeds)\n",
    "rewardbench_general(\"exp/configs/2024_08_22_paper/001_longfact/001_gpt4o_baselines\")\n",
    "# alpacaeval baseline\n",
    "rewardbench_general(\"exp/configs/2024_08_22_paper/001_longfact/002_ae_baseline\")\n",
    "# gpt35 baseline and agent\n",
    "rewardbench_general(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_baseline\")\n",
    "rewardbench_general(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_agent\")\n",
    "# gpt4o agent\n",
    "rewardbench_general(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\")\n",
    "# gemini baseline (currently very rate limited, need other key to run these experiments)\n",
    "# rewardbench_general(\"exp/configs/2024_08_22_paper/001_longfact/003_gemini_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: TruthfulQA\n",
    "# Experiment: Rewardbench non-code, non-maths\n",
    "# gpt4o baselines\n",
    "data_name=\"truthful-qa-v2\"\n",
    "exp_name=\"truthfulqa\"\n",
    "truthfulqa = partial(print_ageval_cmd, experiment_name=exp_name, data_name=data_name)\n",
    "truthfulqa(\"exp/configs/2024_08_22_paper/001_longfact/001_gpt4o_baselines\")\n",
    "# alpacaeval baseline\n",
    "truthfulqa(\"exp/configs/2024_08_22_paper/001_longfact/002_ae_baseline\")\n",
    "# gpt35 baseline and agent\n",
    "truthfulqa(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_baseline\")\n",
    "truthfulqa(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_agent\")\n",
    "# gpt4o agent\n",
    "truthfulqa(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\")\n",
    "# gemini baseline (currently very rate limited, need other key to run these experiments)\n",
    "# truthfulqa(\"exp/configs/2024_08_22_paper/001_longfact/003_gemini_baseline\", exp_name, data_name=data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Rewardbench code\n",
    "# gpt4o baselines\n",
    "data_name=\"hep-python, hep-cpp, hep-go, hep-java, hep-js, hep-rust\"\n",
    "exp_name=\"rewardbench_code\"\n",
    "rewardbench_code = partial(print_ageval_cmd, experiment_name=exp_name, data_name=data_name)\n",
    "rewardbench_code(\"exp/configs/2024_08_22_paper/001_longfact/001_gpt4o_baselines\")\n",
    "# alpacaeval baseline\n",
    "rewardbench_code(\"exp/configs/2024_08_22_paper/001_longfact/002_ae_baseline\")\n",
    "# gpt35 baseline and agent\n",
    "rewardbench_code(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_baseline\")\n",
    "rewardbench_code(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_agent\")\n",
    "# gpt4o agent\n",
    "rewardbench_code(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\")\n",
    "# gemini baseline (currently very rate limited, need other key to run these experiments)\n",
    "# rewardbench_code(\"exp/configs/2024_08_22_paper/001_longfact/003_gemini_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Rewardbench maths\n",
    "# gpt4o baselines\n",
    "data_name=\"math-prm\"\n",
    "exp_name=\"rewardbench_math\"\n",
    "rewardbench_math = partial(print_ageval_cmd, experiment_name=exp_name, data_name=data_name)\n",
    "rewardbench_math(\"exp/configs/2024_08_22_paper/001_longfact/001_gpt4o_baselines\")\n",
    "# alpacaeval baseline\n",
    "rewardbench_math(\"exp/configs/2024_08_22_paper/001_longfact/002_ae_baseline\")\n",
    "# gpt35 baseline and agent\n",
    "rewardbench_math(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_baseline\")\n",
    "rewardbench_math(\"exp/configs/2024_08_22_paper/001_longfact/004_gpt35t_agent\")\n",
    "# gpt4o agent\n",
    "rewardbench_math(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\")\n",
    "# gemini baseline (currently very rate limited, need other key to run these experiments)\n",
    "# rewardbench_math(\"exp/configs/2024_08_22_paper/001_longfact/003_gemini_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "---\n",
    "#### Ablatations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: always using all tools (removing the reverting to baseline)\n",
    "n_seeds = 2\n",
    "long_fact(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_always_tools\", other_args=['evaluator_kwargs.tools_to_always_run=[\"fact_check\", \"code_interpreter\", \"math_checker\"]'], n_seeds=2)\n",
    "gsm8k_math(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_always_tools\", other_args=['evaluator_kwargs.tools_to_always_run=[\"fact_check\", \"code_interpreter\", \"math_checker\"]'], n_seeds=2)\n",
    "rewardbench_general(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_always_tools\", other_args=['evaluator_kwargs.tools_to_always_run=[\"fact_check\", \"code_interpreter\", \"math_checker\"]'], n_seeds=2)\n",
    "rewardbench_code(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_always_tools\", other_args=['evaluator_kwargs.tools_to_always_run=[\"fact_check\", \"code_interpreter\", \"math_checker\"]'], n_seeds=2)\n",
    "rewardbench_math(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_always_tools\", other_args=['evaluator_kwargs.tools_to_always_run=[\"fact_check\", \"code_interpreter\", \"math_checker\"]'], n_seeds=2)\n",
    "truthfulqa(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_always_tools\", other_args=['evaluator_kwargs.tools_to_always_run=[\"fact_check\", \"code_interpreter\", \"math_checker\"]'], n_seeds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: use the arena_hard prompt-type, but in the agent.\n",
    "long_fact(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_ae_prompt\", prompt_type=\"arena_hard\", n_seeds=2)\n",
    "gsm8k_math(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_ae_prompt\", prompt_type=\"arena_hard\", n_seeds=2)\n",
    "rewardbench_code(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_ae_prompt\", prompt_type=\"arena_hard\", n_seeds=2)\n",
    "rewardbench_general(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_ae_prompt\", prompt_type=\"arena_hard\", n_seeds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: don't fallback to the baseline, just run without tool output.\n",
    "long_fact(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_nofallback\", other_args=['evaluator_kwargs.baseline=None'], n_seeds=2)\n",
    "gsm8k_math(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_nofallback\", other_args=['evaluator_kwargs.baseline=None'], n_seeds=2)\n",
    "rewardbench_code(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_nofallback\", other_args=['evaluator_kwargs.baseline=None'], n_seeds=2)\n",
    "rewardbench_general(\"exp/configs/2024_08_22_paper/001_longfact/005_gpt4o_agent\", output_file=\"005_gpt4o_agent_nofallback\", other_args=['evaluator_kwargs.baseline=None'], n_seeds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not implemented yet\n",
    "# Experiment: replacing external validation with feedback from the same LLM (chain-of-thought vs external validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: running GPT-4o OpenAI assistant with code interpreter and web search activated as baseline (test of whether our scaffolding adds anything)\n",
    "model_kwargs = '{tools: [\"code_interpreter\", \"search\"]}'\n",
    "config = \"exp/configs/2024_08_22_paper/001_longfact/006_gpt4o_assistant_baseline\"\n",
    "long_fact(config, output_file=\"006_gpt4o_assistant_baseline\", other_args=['evaluator_kwargs.baseline=None', f'+evaluator_kwargs.model_kwargs={model_kwargs}'], n_seeds=1)\n",
    "gsm8k_math(config, output_file=\"006_gpt4o_assistant_baseline\", other_args=['evaluator_kwargs.baseline=None', f'+evaluator_kwargs.model_kwargs={model_kwargs}'], n_seeds=1)\n",
    "rewardbench_code(config, output_file=\"006_gpt4o_assistant_baseline\", other_args=['evaluator_kwargs.baseline=None', f'+evaluator_kwargs.model_kwargs={model_kwargs}'], n_seeds=1)\n",
    "rewardbench_general(config, output_file=\"006_gpt4o_assistant_baseline\", other_args=['evaluator_kwargs.baseline=None', f'+evaluator_kwargs.model_kwargs={model_kwargs}'], n_seeds=1)\n",
    "\n",
    "# Experiment: running GPT-4o OpenAI assistant with code interpreter activated as baseline (test of whether our scaffolding adds anything)\n",
    "model_kwargs = '{tools: [\"code_interpreter\"]}'\n",
    "long_fact(config, output_file=\"006_gpt4o_assistant_baseline_codeonly\", other_args=['evaluator_kwargs.baseline=None', f'+evaluator_kwargs.model_kwargs={model_kwargs}'], n_seeds=1)\n",
    "gsm8k_math(config, output_file=\"006_gpt4o_assistant_baseline_codeonly\", other_args=['evaluator_kwargs.baseline=None', f'+evaluator_kwargs.model_kwargs={model_kwargs}'], n_seeds=1)\n",
    "rewardbench_code(config, output_file=\"006_gpt4o_assistant_baseline_codeonly\", other_args=['evaluator_kwargs.baseline=None', f'+evaluator_kwargs.model_kwargs={model_kwargs}'], n_seeds=1)\n",
    "rewardbench_general(config, output_file=\"006_gpt4o_assistant_baseline_codeonly\", other_args=['evaluator_kwargs.baseline=None', f'+evaluator_kwargs.model_kwargs={model_kwargs}'], n_seeds=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Post-processing\n",
    "\n",
    "Create 'virtual agents', we don't need to re-run anything, but we combine data to replace the baseline models where fallback is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ageval.analysis.post_processing\n",
    "import ageval.analysis.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELETE_EXISTING_SYNTHETIC_AGENTS = True\n",
    "multirun_paths = {\n",
    "    \"rewardbench_general/agent_synth\": [\n",
    "        DIR / \"rewardbench_general\" / \"005_gpt4o_agent\",\n",
    "        DIR / \"rewardbench_general\" / \"001_gpt4o_baselines\",\n",
    "        DIR / \"rewardbench_general\" / \"002_ae_baseline\",\n",
    "        DIR / \"rewardbench_general\" / \"004_gpt35t_baseline\",\n",
    "        DIR / \"rewardbench_general\" / \"004_gpt35t_agent\",\n",
    "    ],\n",
    "    \"rewardbench_code/agent_synth\": [\n",
    "        DIR / \"rewardbench_code\" / \"005_gpt4o_agent\",\n",
    "        DIR / \"rewardbench_code\" / \"001_gpt4o_baselines\",\n",
    "        DIR / \"rewardbench_code\" / \"002_ae_baseline\",\n",
    "        DIR / \"rewardbench_code\" / \"004_gpt35t_baseline\",\n",
    "        DIR / \"rewardbench_code\" / \"004_gpt35t_agent\",\n",
    "    ],\n",
    "    \"rewardbench_math/agent_synth\": [\n",
    "        DIR / \"rewardbench_math\" / \"005_gpt4o_agent\",\n",
    "        DIR / \"rewardbench_math\" / \"001_gpt4o_baselines\",\n",
    "        DIR / \"rewardbench_math\" / \"002_ae_baseline\",\n",
    "        DIR / \"rewardbench_math\" / \"004_gpt35t_baseline\",\n",
    "        DIR / \"rewardbench_math\" / \"004_gpt35t_agent\",\n",
    "    ],\n",
    "    \"truthfulqa/agent_synth\": [\n",
    "        DIR / \"truthfulqa\" / \"005_gpt4o_agent\",\n",
    "        DIR / \"truthfulqa\" / \"001_gpt4o_baselines\",\n",
    "        DIR / \"truthfulqa\" / \"002_ae_baseline\",\n",
    "        DIR / \"truthfulqa\" / \"004_gpt35t_baseline\",\n",
    "        DIR / \"truthfulqa\" / \"004_gpt35t_agent\",\n",
    "    ],\n",
    "    \"long_fact/agent_synth\": [\n",
    "        DIR / \"long_fact\" / \"005_gpt4o_agent\",\n",
    "        DIR / \"long_fact\" / \"001_gpt4o_baselines\",\n",
    "        DIR / \"long_fact\" / \"002_ae_baseline\",\n",
    "        DIR / \"long_fact\" / \"004_gpt35t_baseline\",\n",
    "        DIR / \"long_fact\" / \"004_gpt35t_agent\",\n",
    "    ],\n",
    "    \"gsm8k_cot_hard/agent_synth\": [\n",
    "        DIR / \"gsm8k_cot_hard\" / \"005_gpt4o_agent\",\n",
    "        DIR / \"gsm8k_cot_hard\" / \"001_gpt4o_baselines\",\n",
    "        DIR / \"gsm8k_cot_hard\" / \"002_ae_baseline\",\n",
    "        DIR / \"gsm8k_cot_hard\" / \"004_gpt35t_baseline\",\n",
    "        DIR / \"gsm8k_cot_hard\" / \"004_gpt35t_agent\",\n",
    "    ],\n",
    "    \"apps_competitive_gpt4/agent_synth\": [\n",
    "        DIR / \"apps_competitive_gpt4\" / \"005_gpt4o_agent\",\n",
    "        DIR / \"apps_competitive_gpt4\" / \"001_gpt4o_baselines\",\n",
    "        DIR / \"apps_competitive_gpt4\" / \"002_ae_baseline\",\n",
    "        DIR / \"apps_competitive_gpt4\" / \"004_gpt35t_baseline\",\n",
    "        DIR / \"apps_competitive_gpt4\" / \"004_gpt35t_agent\",\n",
    "    ],\n",
    "}\n",
    "agent_name=\"agent_gpt-4o-2024-05-13_fact_check_code_interpreter_math_checker_base-basic\"\n",
    "\n",
    "progress_bar = tqdm(multirun_paths.items())\n",
    "for output_dir, paths in progress_bar:\n",
    "    progress_bar.set_description(output_dir)\n",
    "    if DELETE_EXISTING_SYNTHETIC_AGENTS and (DIR / output_dir).exists():\n",
    "        shutil.rmtree(DIR / output_dir)\n",
    "    multirun_path = \", \".join(str(p) for p in paths)\n",
    "    ageval.analysis.post_processing.generate_synth_agent_from_multirun(multirun_path=multirun_path, save_dir=DIR / output_dir, agent_name=agent_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Plotting\n",
    "\n",
    "For each folder that has been created, create the plots for the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ageval.analysis\n",
    "import ageval.analysis.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create human data mapping.\n",
    "human_data = {\n",
    "    \"long_fact\": {\n",
    "        \"model\": \"Human (3 annotators)\",\n",
    "        \"Agreement rate\": None,\n",
    "        \"Agreement rate (incl NAs)\": None,\n",
    "        \"Agreed\": None,\n",
    "        \"Not agreed\": None,\n",
    "        \"Num text_a evaluator pref\": None,\n",
    "        \"Num text_b evaluator pref\": None,\n",
    "        \"No evaluator annotation available\": None,\n",
    "        \"Not avail.\": None,\n",
    "        \"Sum\": None,\n",
    "        \"Agreed (%)\": 76.82926829268293,\n",
    "        \"Not agreed (%)\": 0.0,\n",
    "        \"Not avail. (%)\": 0.0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_toplevel = [os.path.join(DIR, d) for d in os.listdir(DIR) if os.path.isdir(os.path.join(DIR, d)) if d != \"plots\"]\n",
    "for experiment_toplevel in experiments_toplevel:\n",
    "    exp_name_base = os.path.basename(experiment_toplevel)\n",
    "    human_df = pd.DataFrame([human_data[exp_name_base]]) if exp_name_base in human_data else None\n",
    "    individual_exps = [str(dir) for dir in ageval.analysis.data_loader.get_nested_valid_run_parent_dirs(pathlib.Path(experiment_toplevel))]\n",
    "    ageval.experiments.logging.analyze_combined_results(multirun_path=\",\".join(individual_exps), save_path=DIR / \"plots\" / os.path.basename(experiment_toplevel), additional_results=human_df, models_to_hide=[\"agent_gpt-4o-2024-05-13_synthetic_base-5-turbo-0125\", \"agent_gpt-4o-2024-05-13_synthetic_base-basic_gpt-4o-2024-05-13\", \"agent_gpt-4o-2024-05-13_synthetic_base-basic_gpt-3.5-turbo-0125\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
